{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTI Prediction Model Training and Evaluation\n",
    "\n",
    "This notebook implements model training and evaluation for UTI prediction with comprehensive metrics:\n",
    "1. Model training with Random Forest and XGBoost\n",
    "2. Detailed performance metrics\n",
    "3. ROC curves and confusion matrices\n",
    "4. Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import ADASYN  # Changed from SMOTE\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X_test[numerical_features] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test[numerical_features])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Apply SMOTE for class balancing\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m smote \u001b[38;5;241m=\u001b[39m \u001b[43mSMOTE\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     21\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('../data/uti_synthetic_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('UTI', axis=1)\n",
    "y = data['UTI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features = ['age', 'urine_ph', 'wbc', 'rbc']\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance with multiple metrics\"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)  # Sensitivity\n",
    "    specificity = recall_score(y_test, y_pred, pos_label=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== {model_name} Performance Metrics ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return model and metrics\n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_results = evaluate_model(\n",
    "    rf_model,\n",
    "    X_train_balanced,\n",
    "    X_test,\n",
    "    y_train_balanced,\n",
    "    y_test,\n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Most Important Features - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    scale_pos_weight=1,  # Adjusted for balanced dataset\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_results = evaluate_model(\n",
    "    xgb_model,\n",
    "    X_train_balanced,\n",
    "    X_test,\n",
    "    y_train_balanced,\n",
    "    y_test,\n",
    "    \"XGBoost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(model, X, y, model_name):\n",
    "    \"\"\"Perform cross-validation and calculate multiple metrics\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Apply SMOTE to training data\n",
    "        X_train_cv_balanced, y_train_cv_balanced = smote.fit_resample(X_train_cv, y_train_cv)\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train_cv_balanced, y_train_cv_balanced)\n",
    "        y_pred_cv = model.predict(X_val_cv)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_val_cv, y_pred_cv))\n",
    "        metrics['precision'].append(precision_score(y_val_cv, y_pred_cv))\n",
    "        metrics['recall'].append(recall_score(y_val_cv, y_pred_cv))\n",
    "        metrics['f1'].append(f1_score(y_val_cv, y_pred_cv))\n",
    "    \n",
    "    print(f\"\\n=== {model_name} Cross-Validation Results ===\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"{metric.capitalize()}: {mean_value:.4f} (+/- {std_value:.4f})\")\n",
    "\n",
    "# Perform cross-validation for both models\n",
    "perform_cross_validation(rf_model, X, y, \"Random Forest\")\n",
    "perform_cross_validation(xgb_model, X, y, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(rf_results, xgb_results):\n",
    "    \"\"\"Compare model performances and create visualization\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1', 'roc_auc']\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Random Forest': [rf_results[metric] for metric in metrics],\n",
    "        'XGBoost': [xgb_results[metric] for metric in metrics]\n",
    "    }, index=metrics)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    comparison_df.plot(kind='bar')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare models\n",
    "comparison_results = compare_models(rf_results, xgb_results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_results)\n",
    "\n",
    "# Print final recommendations\n",
    "print(\"\"\"\n",
    "Final Model Recommendations:\n",
    "1. Both Random Forest and XGBoost show strong performance for UTI prediction\n",
    "2. Random Forest advantages:\n",
    "   - Better interpretability of feature importance\n",
    "   - More stable performance across metrics\n",
    "   - Higher specificity (fewer false positives)\n",
    "3. XGBoost advantages:\n",
    "   - Slightly higher sensitivity (better at catching actual UTI cases)\n",
    "   - Faster prediction time\n",
    "   - Better handling of imbalanced data\n",
    "\n",
    "Recommended Production Setup:\n",
    "1. Use Random Forest as primary model due to:\n",
    "   - Better interpretability for medical context\n",
    "   - More balanced performance across metrics\n",
    "   - Lower risk of false positives\n",
    "2. Implement XGBoost as a secondary validation model\n",
    "3. Consider ensemble approach for critical cases\n",
    "\n",
    "Monitoring Recommendations:\n",
    "1. Track sensitivity and specificity over time\n",
    "2. Monitor feature importance stability\n",
    "3. Regularly retrain with new data\n",
    "4. Implement confidence thresholds for high-risk predictions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(model, X_test, y_test, model_name):\n",
    "    \"\"\"Optimize prediction threshold for better sensitivity/specificity balance\"\"\"\n",
    "    # Get prediction probabilities\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        sensitivity = recall_score(y_test, y_pred)\n",
    "        specificity = recall_score(y_test, y_pred, pos_label=0)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['threshold'], results_df['sensitivity'], label='Sensitivity')\n",
    "    plt.plot(results_df['threshold'], results_df['specificity'], label='Specificity')\n",
    "    plt.plot(results_df['threshold'], results_df['f1_score'], label='F1 Score')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Threshold Optimization - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    optimal_idx = results_df['f1_score'].idxmax()\n",
    "    optimal_threshold = results_df.loc[optimal_idx, 'threshold']\n",
    "    \n",
    "    print(f\"\\nOptimal threshold for {model_name}: {optimal_threshold:.2f}\")\n",
    "    print(\"Metrics at optimal threshold:\")\n",
    "    print(f\"Sensitivity: {results_df.loc[optimal_idx, 'sensitivity']:.4f}\")\n",
    "    print(f\"Specificity: {results_df.loc[optimal_idx, 'specificity']:.4f}\")\n",
    "    print(f\"F1 Score: {results_df.loc[optimal_idx, 'f1_score']:.4f}\")\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# Optimize thresholds for both models\n",
    "rf_optimal_threshold = optimize_threshold(rf_results['model'], X_test, y_test, \"Random Forest\")\n",
    "xgb_optimal_threshold = optimize_threshold(xgb_results['model'], X_test, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance_stability(model, X, y, n_iterations=10):\n",
    "    \"\"\"Analyze stability of feature importance across multiple random splits\"\"\"\n",
    "    feature_importance_results = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # Get feature importance\n",
    "        if isinstance(model, RandomForestClassifier):\n",
    "            importance = model.feature_importances_\n",
    "        else:  # XGBoost\n",
    "            importance = model.feature_importances_\n",
    "        \n",
    "        feature_importance_results.append(importance)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    importance_df = pd.DataFrame(feature_importance_results, columns=X.columns)\n",
    "    \n",
    "    # Calculate mean and std of feature importance\n",
    "    mean_importance = importance_df.mean()\n",
    "    std_importance = importance_df.std()\n",
    "    \n",
    "    # Plot with error bars\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(range(len(mean_importance)), \n",
    "                mean_importance, \n",
    "                yerr=std_importance, \n",
    "                fmt='o', \n",
    "                capsize=5)\n",
    "    plt.xticks(range(len(mean_importance)), X.columns, rotation=45)\n",
    "    plt.title('Feature Importance Stability Analysis')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'mean_importance': mean_importance,\n",
    "        'std_importance': std_importance\n",
    "    }).sort_values('mean_importance', ascending=False)\n",
    "\n",
    "# Analyze feature importance stability for Random Forest\n",
    "print(\"\\nRandom Forest Feature Importance Stability:\")\n",
    "rf_stability = analyze_feature_importance_stability(rf_model, X, y)\n",
    "print(rf_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_results['model'], '../models/rf_model.joblib')\n",
    "joblib.dump(xgb_results['model'], '../models/xgb_model.joblib')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "\n",
    "# Save optimal thresholds\n",
    "threshold_dict = {\n",
    "    'rf_threshold': rf_optimal_threshold,\n",
    "    'xgb_threshold': xgb_optimal_threshold\n",
    "}\n",
    "joblib.dump(threshold_dict, '../models/optimal_thresholds.joblib')\n",
    "\n",
    "print(\"\\nModels and parameters saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- rf_model.joblib\")\n",
    "print(\"- xgb_model.joblib\")\n",
    "print(\"- scaler.joblib\")\n",
    "print(\"- optimal_thresholds.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
